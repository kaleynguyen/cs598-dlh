{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install polars numpy torch pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# set seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "DATA_PATH = \"\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "obj = pickle.load(open(os.path.join(DATA_PATH, 'allevents_by_episode_48_charts'), 'rb'))\n",
    "patients = obj.select(pl.col('subject_id')).to_series().to_list()\n",
    "seqs = obj.select(pl.col('itemidx')).to_series().to_list()\n",
    "mortality = obj.select(pl.col('mortality_tf')).to_series().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40119"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemidx = set([each_item for events in seqs for items in events for each_item in items])\n",
    "len(itemidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, seqs, mortality):\n",
    "        self.seqs = seqs\n",
    "        self.labels = mortality\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, index):\n",
    "        return self.seqs[index], self.labels[index]\n",
    "dataset = CustomDataset(seqs, mortality)\n",
    "assert len(dataset) == len(obj) #TODO write test separately \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function\n",
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Input data: a tuple of seqs, and label\n",
    "    Output: x (num_patients, num_events, num_itemids) with *real* items in the events\n",
    "            rev_x (num_patients, num_events, num_itemids) with *reverse real* items in the events\n",
    "            masks  (num_patients, num_events, num_itemids) whether a *real* itemid in the events is present\n",
    "            rev_masks (num_patients, num_events, num_itemids) whether a *reverse real* itemid in the events is present\n",
    "    \"\"\"\n",
    "    seqs, labels = zip(*data)\n",
    "    num_patients = len(seqs)\n",
    "    max_num_events = max([len(event) for event in seqs])\n",
    "    max_num_items = max([len(itemid) for event in seqs for itemid in event])\n",
    "    tensor_shape = (num_patients, max_num_events, max_num_items)\n",
    "    x =        torch.zeros(tensor_shape, dtype=torch.long)\n",
    "    rev_x =    torch.zeros(tensor_shape, dtype=torch.long)\n",
    "    masks =    torch.zeros(tensor_shape, dtype=torch.bool)\n",
    "    rev_masks = torch.zeros(tensor_shape, dtype=torch.bool) \n",
    "    y =        torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "\n",
    "    for i_patient, events in enumerate(seqs):\n",
    "        for i_event, item in enumerate(events):\n",
    "            padded_item = torch.concat([torch.tensor(item),\n",
    "                                        torch.zeros(max_num_items - len(item))]).long()\n",
    "            x[i_patient, i_event, :] = padded_item\n",
    "            masks[i_patient, i_event, :] = torch.where(padded_item!=0,1,0)  \n",
    "    for i_patient, events in enumerate(seqs):\n",
    "        idx_all_real_events = torch.sum(x[i_patient, :, :], dim=(1))!= 0\n",
    "        idx_padded_events =torch.sum(x[i_patient, :, :], dim=(1))== 0\n",
    "        fliped = torch.flip(x[i_patient, idx_all_real_events, :].unsqueeze(1), (0,)).squeeze(1)\n",
    "        rev_x[i_patient, :, :] = torch.concat((fliped, x[i_patient, idx_padded_events, :] ))\n",
    "        rev_masks[i_patient, :, :] = torch.where(rev_x[i_patient, :, :] != 0, True, False)\n",
    "    return x, masks, rev_x, rev_masks, y\n",
    "\n",
    "    \n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(dataset, batch_size=10, collate_fn=collate_fn, pin_memory=True)\n",
    "loader_iter = iter(loader)\n",
    "x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "#assert x.shape == masks.shape == (10, 8, 313)\n",
    "assert y.shape == (10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 27032\n",
      "Length of train with mortality: 3542\n",
      "Length of train with survive patients: 23490\n",
      "Length of val dataset: 3379\n",
      "Length of val with mortality: 458\n",
      "Length of val with survive patients: 26574\n",
      "Length of test dataset: 3379\n",
      "Length of test with mortality: 471\n",
      "Length of test with survive patients: 2908\n"
     ]
    }
   ],
   "source": [
    "train, val, test = int(len(dataset)*0.8), int(len(dataset)*0.1), len(dataset) - int(len(dataset)*0.8) -  int(len(dataset)*0.1)\n",
    "lengths = [train, val, test]\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset=dataset, lengths=lengths)\n",
    "from pytorch_metric_learning import samplers\n",
    "\n",
    "def load_data(train_dataset, val_dataset, test_dataset, collate_fn):\n",
    "    batch_size = 64\n",
    "    seqs, labels = zip(*train_dataset)\n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, \\\n",
    "                                collate_fn=collate_fn, \\\n",
    "                                num_workers=4, shuffle=True)\n",
    "    \n",
    "    val_loader = DataLoader(train_dataset, batch_size=batch_size, \\\n",
    "                                collate_fn=collate_fn, \\\n",
    "                                num_workers=4, shuffle=False)\n",
    "    test_loader = DataLoader(train_dataset, batch_size=batch_size, \\\n",
    "                                collate_fn=collate_fn, \\\n",
    "                                num_workers=4, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset, test_dataset, collate_fn)\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of train with mortality:\", np.sum([element[-1] for element in train_dataset]))\n",
    "print(\"Length of train with survive patients:\", (len(train_dataset) - np.sum([element[-1] for element in train_dataset])))\n",
    "print(\"Length of val dataset:\", len(val_dataset))\n",
    "print(\"Length of val with mortality:\", np.sum([element[-1] for element in val_dataset]))\n",
    "print(\"Length of val with survive patients:\", (len(train_dataset) - np.sum([element[-1] for element in val_dataset])))\n",
    "print(\"Length of test dataset:\", len(test_dataset))\n",
    "print(\"Length of test with mortality:\", np.sum([element[-1] for element in test_dataset]))\n",
    "print(\"Length of test with survive patients:\", (len(test_dataset) - np.sum([element[-1] for element in test_dataset])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3542\n"
     ]
    }
   ],
   "source": [
    "def custom_for_loop(iterable):\n",
    "    iterator = iter(iterable)\n",
    "    done_looping = False\n",
    "    count = 0\n",
    "    while not done_looping:\n",
    "        try:\n",
    "            item = next(iterator)\n",
    "        except StopIteration:\n",
    "            done_looping = True\n",
    "        else:\n",
    "            x, masks, rev_x, rev_masks, y = item\n",
    "            count += y.sum().item()\n",
    "    print(count)\n",
    "\n",
    "custom_for_loop(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_masks(x, masks):\n",
    "    \"\"\"\n",
    "    Input:  x               (batch_size, num_events, num_itemids, embedding_dims)\n",
    "            mask            (batch_size, num_events, num_itemids)\n",
    "    Output: sum_embeddings  (batch_size, num_events, embedding_dims)\n",
    "    The return output compress the num_itemids into embedding vectors\n",
    "    \"\"\"\n",
    "    masks = masks.unsqueeze(-1).expand(x.shape[0], x.shape[1], x.shape[2], x.shape[3])\n",
    "    return torch.sum(masks * x, 2)\n",
    "def get_last_event(hidden_states, masks):\n",
    "    \"\"\"\n",
    "    hidden_states: (batch_size, #item, embedding_dim)\n",
    "    masks:         (batch_size, #item, embedding_dim)\n",
    "    return last_hidden_state: (batch_size, embedding_dim)\n",
    "    \"\"\"\n",
    "    idx_last_event = torch.argmin(torch.sum(masks, 2), 1)\n",
    "    return hidden_states[torch.arange(hidden_states.shape[0]), torch.where(idx_last_event - 1 < 0, max(idx_last_event), idx_last_event - 1),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = torch.tensor([[\n",
    "    [-0.8201, 0.3956, 0.8989, -1.3884, -0.1670, 0.2851, -0.6411],\n",
    "    [-0.8937, 0.9265, -0.5355, -1.1597, -0.4602, 0.7085, 1.0128],\n",
    "    [ 0.2304, 1.0902, -1.5827, -0.3246, 1.9264, -0.3300, 0.1984]],\n",
    "   \n",
    "[[ 0.7821, 1.0391, -0.7245, -0.2093, -0.2153, -1.8157, -0.3452],\n",
    "    [-2.0615, 0.6741, -1.3233, -1.3598, -0.0835, -0.0235, 0.1744],\n",
    "    [ 2.2983, 0.9571, -0.6619, -0.8285, -0.6057, -1.4013, 1.2973]],\n",
    "\n",
    "   [[ 1.6409, -1.0567, -0.2616, -0.2501, 0.5011, 0.2600, -0.1782],\n",
    "    [    -0.2595, -0.0145, -0.3839, -2.9662, -1.0606, -0.3090, 0.9343],\n",
    "    [ 1.6243, 0.0016, -0.4375, -2.1085, 1.1450, -0.3822, -0.3553]],\n",
    "\n",
    "   [[ 0.7542, 0.1332, 0.1825, -0.5146, 0.8005, -0.1259, -0.9578],\n",
    "    [ 1.7518, 0.9796, 0.4105, 1.7675, -0.0832, 0.5087, -0.8253],\n",
    "    [ 0.1633, 0.5013, 1.4206, 1.1542, -1.5366, -0.5577, -0.4383]]])\n",
    "masks = torch.tensor([[[ True,  True,  True,  True, False],\n",
    "         [ True,  True,  True, False, False],\n",
    "         [False, False, False, False, False]],\n",
    "\n",
    "        [[ True, False, False, False, False],\n",
    "         [ True,  True,  True,  True, False],\n",
    "         [ True,  True, False, False, False]],\n",
    "\n",
    "        [[ True,  True, False, False, False],\n",
    "         [False, False, False, False, False],\n",
    "         [False, False, False, False, False]],\n",
    "\n",
    "        [[ True,  True,  True, False, False],\n",
    "         [ True,  True,  True, False, False],\n",
    "         [False, False, False, False, False]]])\n",
    "assert torch.sum(get_last_event(ts, masks) == torch.tensor([\n",
    "    [-0.8937, 0.9265, -0.5355, -1.1597, -0.4602, 0.7085, 1.0128],\n",
    "    [ 2.2983, 0.9571, -0.6619, -0.8285, -0.6057, -1.4013, 1.2973],\n",
    "    [1.6409, -1.0567, -0.2616, -0.2501, 0.5011, 0.2600, -0.1782],\n",
    "    [ 1.7518, 0.9796, 0.4105, 1.7675, -0.0832, 0.5087, -0.8253],\n",
    "    ])) == 7*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaiveRNN(\n",
       "  (embedding): Embedding(40119, 128)\n",
       "  (rnn): GRU(128, 128, batch_first=True)\n",
       "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive RNN hehe :P\n",
    "class NaiveRNN(nn.Module):\n",
    "    def __init__(self, num_items):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_items, embedding_dim=128)\n",
    "        self.rnn = nn.GRU(128, hidden_size=128, batch_first=True)\n",
    "        self.rev_rnn = nn.GRU(128, hidden_size=128, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=128*2, out_features=1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        batch_size = x.shape[0]\n",
    "        # Forward pass with x and masks\n",
    "        x = self.dropout(self.embedding(x))\n",
    "        x = sum_embeddings_with_masks(x, masks)\n",
    "        output, _ = self.rnn(x)\n",
    "        real_event_h = get_last_event(output, masks)\n",
    "\n",
    "        # Forward pass with rev_x and rev_masks\n",
    "        rev_x = self.dropout(self.embedding(rev_x))\n",
    "        rev_x = sum_embeddings_with_masks(rev_x, rev_masks)\n",
    "        output_rev, _ = self.rnn(rev_x)\n",
    "        real_event_h_rev = get_last_event(output_rev, rev_masks)\n",
    "\n",
    "        # Concat both hidden states\n",
    "        logits = self.dropout(self.fc(torch.cat([real_event_h, real_event_h_rev], 1)))\n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs.view(batch_size)\n",
    "naive_rnn = NaiveRNN(num_items=len(itemidx))\n",
    "naive_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8689701094998521, 0.13102989050014796)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, num_majority, num_minority, K=0.01):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        \n",
    "        self.num_majority = num_majority\n",
    "        self.num_minority = num_minority\n",
    "        self.K = K\n",
    "    def forward(self, y_hat, y):\n",
    "        #y = torch.to('cpu').LongTensor(y)\n",
    "        delta_maj, delta_min = self.K / self.num_majority**(1/4), self.K / self.num_minority**(1/4)\n",
    "        zj = (y_hat > 0.5).int()\n",
    "        masks = torch.where(zj != y)\n",
    "        zc = y_hat\n",
    "        sigma_zc_maj = torch.exp(y_hat - delta_maj) / \\\n",
    "                            (torch.exp(y_hat - delta_maj) + torch.sum(torch.exp(zj[masks])))\n",
    "\n",
    "        sigma_zc_min = torch.exp(y_hat - delta_min) / \\\n",
    "                            (torch.exp(y_hat - delta_min) + torch.sum(torch.exp(zj[masks])))\n",
    "\n",
    "        loss = (- torch.log(sigma_zc_maj) - torch.log(sigma_zc_min)).mean()\n",
    "        return loss\n",
    "minority = np.sum([element[-1] for element in train_dataset]) / len(train_dataset)\n",
    "majority = 1 - minority\n",
    "(majority, minority)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CustomLoss(num_majority=majority, num_minority=minority)\n",
    "optimizer = torch.optim.Adam(naive_rnn.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import *\n",
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.to(device).train()\n",
    "        train_loss = 0\n",
    "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
    "            x, masks, rev_x, rev_masks, y = x.to(device), masks.to(device), rev_x.to(device), rev_masks.to(device), y.to(device)\n",
    "            y_hat = model(x, masks, rev_x, rev_masks).view(y.shape[0])\n",
    "            loss = criterion(y_hat, y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        acc, auc, precision, recall, f1score = eval_model(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation acc: {:.2f}, auc:{:.2f}, precision: {:.2f}, recall: {:.2f}, f1: {:.2f}' \n",
    "              .format(epoch+1, acc, auc, precision, recall, f1score))    \n",
    "def classification_metrics(Y_score, Y_pred, Y_true):\n",
    "    acc, auc, precision, recall, f1score = accuracy_score(Y_true, Y_pred), \\\n",
    "                                           roc_auc_score(Y_true, Y_score), \\\n",
    "                                           precision_score(Y_true, Y_pred), \\\n",
    "                                           recall_score(Y_true, Y_pred), \\\n",
    "                                           f1_score(Y_true, Y_pred)\n",
    "    return acc, auc, precision, recall, f1score\n",
    "\n",
    "\n",
    "\n",
    "#precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "def eval_model(model, val_loader):\n",
    "    model.to(device).eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
    "        x, masks, rev_x, rev_masks, y = x.to(device), masks.to(device), rev_x.to(device), rev_masks.to(device), y.to(device)\n",
    "        y_true = y.long().detach().to('cpu')\n",
    "        y_hat = model(x, masks, rev_x, rev_masks).view(y.shape[0])\n",
    "        y_score = y_hat.detach().to('cpu')\n",
    "        y_pred = (y_hat > 0.5).int().detach().to('cpu')\n",
    "    acc, auc, precision, recall, f1score = classification_metrics(y_score, y_pred, y_true)\n",
    "    return acc, auc, precision, recall, f1score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 7.885139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:34<28:11, 34.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Validation acc: 0.21, auc:0.55, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 2 \t Training Loss: 7.877917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [01:08<27:22, 34.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \t Validation acc: 0.21, auc:0.67, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 3 \t Training Loss: 7.883903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [01:41<26:16, 33.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \t Validation acc: 0.21, auc:0.66, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 4 \t Training Loss: 7.878412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [02:15<25:50, 33.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \t Validation acc: 0.21, auc:0.61, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 5 \t Training Loss: 7.879451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [02:47<24:47, 33.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \t Validation acc: 0.21, auc:0.63, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 6 \t Training Loss: 7.884040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [03:21<24:28, 33.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \t Validation acc: 0.21, auc:0.64, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 7 \t Training Loss: 7.877442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [03:52<23:27, 32.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \t Validation acc: 0.21, auc:0.64, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 8 \t Training Loss: 7.872674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [04:27<23:19, 33.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 \t Validation acc: 0.21, auc:0.61, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 9 \t Training Loss: 7.878516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [04:57<22:07, 32.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 \t Validation acc: 0.21, auc:0.61, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 10 \t Training Loss: 7.880030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [05:32<22:02, 33.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \t Validation acc: 0.21, auc:0.61, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 11 \t Training Loss: 7.877290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [06:02<20:55, 32.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 \t Validation acc: 0.21, auc:0.62, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 12 \t Training Loss: 7.876742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [06:36<20:51, 32.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 \t Validation acc: 0.21, auc:0.62, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 13 \t Training Loss: 7.874553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [07:07<19:53, 32.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 \t Validation acc: 0.21, auc:0.59, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 14 \t Training Loss: 7.877358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [07:42<19:49, 33.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 \t Validation acc: 0.21, auc:0.58, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 15 \t Training Loss: 7.883629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [08:14<19:01, 32.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 \t Validation acc: 0.21, auc:0.59, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 16 \t Training Loss: 7.879237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [08:48<18:50, 33.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 \t Validation acc: 0.21, auc:0.58, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 17 \t Training Loss: 7.878897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [09:21<18:07, 32.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 \t Validation acc: 0.21, auc:0.57, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 18 \t Training Loss: 7.880668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [09:55<17:53, 33.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 \t Validation acc: 0.21, auc:0.57, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 19 \t Training Loss: 7.879815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [10:28<17:14, 33.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 \t Validation acc: 0.21, auc:0.58, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 20 \t Training Loss: 7.869859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [11:03<16:53, 33.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 \t Validation acc: 0.21, auc:0.59, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 21 \t Training Loss: 7.879163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [11:37<16:23, 33.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 \t Validation acc: 0.21, auc:0.59, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 22 \t Training Loss: 7.872599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [12:12<15:56, 34.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 \t Validation acc: 0.21, auc:0.56, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 23 \t Training Loss: 7.867401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [12:46<15:19, 34.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 \t Validation acc: 0.21, auc:0.58, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 24 \t Training Loss: 7.877571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [13:20<14:43, 33.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 \t Validation acc: 0.21, auc:0.60, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 25 \t Training Loss: 7.871286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [13:54<14:11, 34.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 \t Validation acc: 0.21, auc:0.59, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 26 \t Training Loss: 7.871653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [14:27<13:31, 33.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 \t Validation acc: 0.21, auc:0.58, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 27 \t Training Loss: 7.873672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [15:01<12:57, 33.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 \t Validation acc: 0.21, auc:0.59, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 28 \t Training Loss: 7.872354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [15:33<12:13, 33.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 \t Validation acc: 0.21, auc:0.59, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 29 \t Training Loss: 7.882257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [16:07<11:44, 33.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 \t Validation acc: 0.21, auc:0.57, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 30 \t Training Loss: 7.883868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [16:39<10:56, 32.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 \t Validation acc: 0.21, auc:0.59, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 31 \t Training Loss: 7.877083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [17:13<10:34, 33.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 \t Validation acc: 0.21, auc:0.58, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 32 \t Training Loss: 7.881454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [17:44<09:49, 32.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 \t Validation acc: 0.21, auc:0.59, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 33 \t Training Loss: 7.874670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [18:19<09:27, 33.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 \t Validation acc: 0.21, auc:0.59, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 34 \t Training Loss: 7.884878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [18:50<08:39, 32.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 \t Validation acc: 0.21, auc:0.59, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 35 \t Training Loss: 7.876877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [19:24<08:16, 33.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 \t Validation acc: 0.21, auc:0.59, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 36 \t Training Loss: 7.874306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [19:54<07:31, 32.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 \t Validation acc: 0.21, auc:0.60, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 37 \t Training Loss: 7.869903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [20:29<07:08, 32.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 \t Validation acc: 0.21, auc:0.58, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 38 \t Training Loss: 7.877429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [21:00<06:28, 32.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 \t Validation acc: 0.21, auc:0.58, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 39 \t Training Loss: 7.878741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [21:35<06:03, 33.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 \t Validation acc: 0.21, auc:0.57, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 40 \t Training Loss: 7.871531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [22:06<05:26, 32.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 \t Validation acc: 0.21, auc:0.58, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 41 \t Training Loss: 7.877758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [22:41<04:59, 33.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 \t Validation acc: 0.21, auc:0.59, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 42 \t Training Loss: 7.873730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [23:14<04:25, 33.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 \t Validation acc: 0.21, auc:0.57, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 43 \t Training Loss: 7.878468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [23:49<03:55, 33.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 \t Validation acc: 0.21, auc:0.59, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 44 \t Training Loss: 7.877951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [24:22<03:21, 33.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 \t Validation acc: 0.21, auc:0.56, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 45 \t Training Loss: 7.874108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [24:57<02:49, 33.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 \t Validation acc: 0.21, auc:0.58, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 46 \t Training Loss: 7.883939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [25:31<02:15, 33.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 \t Validation acc: 0.21, auc:0.57, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 47 \t Training Loss: 7.873534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [26:05<01:42, 34.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 \t Validation acc: 0.21, auc:0.58, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 48 \t Training Loss: 7.879897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [26:39<01:07, 34.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 \t Validation acc: 0.21, auc:0.60, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 49 \t Training Loss: 7.873395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [27:08<00:32, 32.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 \t Validation acc: 0.21, auc:0.60, precision: 0.21, recall: 1.00, f1: 0.34\n",
      "Epoch: 50 \t Training Loss: 7.873006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [27:22<00:00, 32.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 \t Validation acc: 0.21, auc:0.59, precision: 0.21, recall: 1.00, f1: 0.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 50\n",
    "train(naive_rnn, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20833333333333334 0.5894736842105264 0.20833333333333334 1.0 <function f1_score at 0x7fcf58795550>\n"
     ]
    }
   ],
   "source": [
    "acc, auc, precision, recall, f1score = eval_model(naive_rnn, val_loader)\n",
    "print(acc, auc, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20833333333333334 0.5894736842105264 0.20833333333333334 1.0 <function f1_score at 0x7fcf58795550>\n"
     ]
    }
   ],
   "source": [
    "acc, auc, precision, recall, f1score = eval_model(naive_rnn, test_loader)\n",
    "print(acc, auc, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

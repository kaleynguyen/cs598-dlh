{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install polars numpy torch pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# set seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "DATA_PATH = \"\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "obj = pickle.load(open(os.path.join(DATA_PATH, 'allevents_by_episode_48_charts'), 'rb'))\n",
    "obj.estimated_size()/(1024**2)\n",
    "patients = obj.select(pl.col('subject_id')).to_series().to_list()\n",
    "seqs = obj.select(pl.col('itemidx')).to_series().to_list()\n",
    "mortality = obj.select(pl.col('mortality_tf')).to_series().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40119"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemidx = set([each_item for events in seqs for items in events for each_item in items])\n",
    "len(itemidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, seqs, mortality):\n",
    "        self.seqs = seqs\n",
    "        self.labels = mortality\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, index):\n",
    "        return self.seqs[index], self.labels[index]\n",
    "dataset = CustomDataset(seqs, mortality)\n",
    "assert len(dataset) == len(obj) #TODO write test separately \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function\n",
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Input data: a tuple of seqs, and label\n",
    "    Output: x (num_patients, num_events, num_itemids) with *real* items in the events\n",
    "            rev_x (num_patients, num_events, num_itemids) with *reverse real* items in the events\n",
    "            masks  (num_patients, num_events, num_itemids) whether a *real* itemid in the events is present\n",
    "            rev_masks (num_patients, num_events, num_itemids) whether a *reverse real* itemid in the events is present\n",
    "    \"\"\"\n",
    "    seqs, labels = zip(*data)\n",
    "    num_patients = len(seqs)\n",
    "    max_num_events = max([len(event) for event in seqs])\n",
    "    max_num_items = max([len(itemid) for event in seqs for itemid in event])\n",
    "    tensor_shape = (num_patients, max_num_events, max_num_items)\n",
    "    x =        torch.zeros(tensor_shape, dtype=torch.long)\n",
    "    rev_x =    torch.zeros(tensor_shape, dtype=torch.long)\n",
    "    masks =    torch.zeros(tensor_shape, dtype=torch.bool)\n",
    "    rev_masks = torch.zeros(tensor_shape, dtype=torch.bool) \n",
    "    y =        torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "\n",
    "    for i_patient, events in enumerate(seqs):\n",
    "        for i_event, item in enumerate(events):\n",
    "            padded_item = torch.concat([torch.tensor(item),\n",
    "                                        torch.zeros(max_num_items - len(item))]).long()\n",
    "            x[i_patient, i_event, :] = padded_item\n",
    "            masks[i_patient, i_event, :] = torch.where(padded_item!=0,1,0)  \n",
    "    for i_patient, events in enumerate(seqs):\n",
    "        idx_all_real_events = torch.sum(x[i_patient, :, :], dim=(1))!= 0\n",
    "        idx_padded_events =torch.sum(x[i_patient, :, :], dim=(1))== 0\n",
    "        fliped = torch.flip(x[i_patient, idx_all_real_events, :].unsqueeze(1), (0,)).squeeze(1)\n",
    "        rev_x[i_patient, :, :] = torch.concat((fliped, x[i_patient, idx_padded_events, :] ))\n",
    "        rev_masks[i_patient, :, :] = torch.where(rev_x[i_patient, :, :] != 0, True, False)\n",
    "    return x, masks, rev_x, rev_masks, y\n",
    "\n",
    "    \n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(dataset, batch_size=10, collate_fn=collate_fn, pin_memory=True)\n",
    "loader_iter = iter(loader)\n",
    "x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "#assert x.shape == masks.shape == (10, 8, 313)\n",
    "assert y.shape == (10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 27032\n",
      "Length of train with mortality: 3552\n",
      "Length of train with survive patients: 23480\n",
      "Length of val dataset: 3379\n",
      "Length of val with mortality: 451\n",
      "Length of val with survive patients: 26581\n",
      "Length of test dataset: 3379\n",
      "Length of test with mortality: 468\n",
      "Length of test with survive patients: 2911\n"
     ]
    }
   ],
   "source": [
    "train, val, test = int(len(dataset)*0.8), int(len(dataset)*0.1), len(dataset) - int(len(dataset)*0.8) -  int(len(dataset)*0.1)\n",
    "lengths = [train, val, test]\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset=dataset, lengths=lengths)\n",
    "import math\n",
    "def load_data(train_dataset, val_dataset, test_dataset, collate_fn):\n",
    "    batch_size = 64\n",
    "    seqs, labels = zip(*train_dataset)\n",
    "    arr = np.array(labels)\n",
    "    #First stage of sampling: find all the positive labels\n",
    "    idx_true = np.where(arr == True)\n",
    "    minority, majority= np.sum(arr)/len(arr), 1 - np.sum(arr)/len(arr)\n",
    "    weights = np.apply_along_axis(lambda x: x * (majority) if x is True else x * minority, 0, np.ones_like(arr))\n",
    "    #sampler = torch.utils.data.sampler.WeightedRandomSampler(weights=weights, num_samples = batch_size, replacement=False)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, \\\n",
    "                                collate_fn=collate_fn, \\\n",
    "                                num_workers=4, shuffle=True)\n",
    "    \n",
    "    val_loader = DataLoader(train_dataset, batch_size=batch_size, \\\n",
    "                                collate_fn=collate_fn, \\\n",
    "                                num_workers=4, shuffle=False)\n",
    "    test_loader = DataLoader(train_dataset, batch_size=batch_size, \\\n",
    "                                collate_fn=collate_fn, \\\n",
    "                                num_workers=4, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset, test_dataset, collate_fn)\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of train with mortality:\", np.sum([element[-1] for element in train_dataset]))\n",
    "print(\"Length of train with survive patients:\", (len(train_dataset) - np.sum([element[-1] for element in train_dataset])))\n",
    "print(\"Length of val dataset:\", len(val_dataset))\n",
    "print(\"Length of val with mortality:\", np.sum([element[-1] for element in val_dataset]))\n",
    "print(\"Length of val with survive patients:\", (len(train_dataset) - np.sum([element[-1] for element in val_dataset])))\n",
    "print(\"Length of test dataset:\", len(test_dataset))\n",
    "print(\"Length of test with mortality:\", np.sum([element[-1] for element in test_dataset]))\n",
    "print(\"Length of test with survive patients:\", (len(test_dataset) - np.sum([element[-1] for element in test_dataset])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_masks(x, masks):\n",
    "    \"\"\"\n",
    "    Input:  x               (batch_size, num_events, num_itemids, embedding_dims)\n",
    "            mask            (batch_size, num_events, num_itemids)\n",
    "    Output: sum_embeddings  (batch_size, num_events, embedding_dims)\n",
    "    The return output compress the num_itemids into embedding vectors\n",
    "    \"\"\"\n",
    "    masks = masks.unsqueeze(-1).expand(x.shape[0], x.shape[1], x.shape[2], x.shape[3])\n",
    "    return torch.sum(masks * x, 2)\n",
    "def get_last_event(hidden_states, masks):\n",
    "    \"\"\"\n",
    "    hidden_states: (batch_size, #item, embedding_dim)\n",
    "    masks:         (batch_size, #item, embedding_dim)\n",
    "    return last_hidden_state: (batch_size, embedding_dim)\n",
    "    \"\"\"\n",
    "    idx_last_event = torch.argmin(torch.sum(masks, 2), 1)\n",
    "    return hidden_states[torch.arange(hidden_states.shape[0]), torch.where(idx_last_event - 1 < 0, max(idx_last_event), idx_last_event - 1),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = torch.tensor([[\n",
    "    [-0.8201, 0.3956, 0.8989, -1.3884, -0.1670, 0.2851, -0.6411],\n",
    "    [-0.8937, 0.9265, -0.5355, -1.1597, -0.4602, 0.7085, 1.0128],\n",
    "    [ 0.2304, 1.0902, -1.5827, -0.3246, 1.9264, -0.3300, 0.1984]],\n",
    "   \n",
    "[[ 0.7821, 1.0391, -0.7245, -0.2093, -0.2153, -1.8157, -0.3452],\n",
    "    [-2.0615, 0.6741, -1.3233, -1.3598, -0.0835, -0.0235, 0.1744],\n",
    "    [ 2.2983, 0.9571, -0.6619, -0.8285, -0.6057, -1.4013, 1.2973]],\n",
    "\n",
    "   [[ 1.6409, -1.0567, -0.2616, -0.2501, 0.5011, 0.2600, -0.1782],\n",
    "    [    -0.2595, -0.0145, -0.3839, -2.9662, -1.0606, -0.3090, 0.9343],\n",
    "    [ 1.6243, 0.0016, -0.4375, -2.1085, 1.1450, -0.3822, -0.3553]],\n",
    "\n",
    "   [[ 0.7542, 0.1332, 0.1825, -0.5146, 0.8005, -0.1259, -0.9578],\n",
    "    [ 1.7518, 0.9796, 0.4105, 1.7675, -0.0832, 0.5087, -0.8253],\n",
    "    [ 0.1633, 0.5013, 1.4206, 1.1542, -1.5366, -0.5577, -0.4383]]])\n",
    "masks = torch.tensor([[[ True,  True,  True,  True, False],\n",
    "         [ True,  True,  True, False, False],\n",
    "         [False, False, False, False, False]],\n",
    "\n",
    "        [[ True, False, False, False, False],\n",
    "         [ True,  True,  True,  True, False],\n",
    "         [ True,  True, False, False, False]],\n",
    "\n",
    "        [[ True,  True, False, False, False],\n",
    "         [False, False, False, False, False],\n",
    "         [False, False, False, False, False]],\n",
    "\n",
    "        [[ True,  True,  True, False, False],\n",
    "         [ True,  True,  True, False, False],\n",
    "         [False, False, False, False, False]]])\n",
    "assert torch.sum(get_last_event(ts, masks) == torch.tensor([\n",
    "    [-0.8937, 0.9265, -0.5355, -1.1597, -0.4602, 0.7085, 1.0128],\n",
    "    [ 2.2983, 0.9571, -0.6619, -0.8285, -0.6057, -1.4013, 1.2973],\n",
    "    [1.6409, -1.0567, -0.2616, -0.2501, 0.5011, 0.2600, -0.1782],\n",
    "    [ 1.7518, 0.9796, 0.4105, 1.7675, -0.0832, 0.5087, -0.8253],\n",
    "    ])) == 7*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaiveRNN(\n",
       "  (embedding): Embedding(40119, 128)\n",
       "  (rnn): GRU(128, 128, batch_first=True)\n",
       "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive RNN hehe :P\n",
    "class NaiveRNN(nn.Module):\n",
    "    def __init__(self, num_items):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_items, embedding_dim=128)\n",
    "        self.rnn = nn.GRU(128, hidden_size=128, batch_first=True)\n",
    "        self.rev_rnn = nn.GRU(128, hidden_size=128, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=128*2, out_features=1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        batch_size = x.shape[0]\n",
    "        # Forward pass with x and masks\n",
    "        x = self.dropout(self.embedding(x))\n",
    "        x = sum_embeddings_with_masks(x, masks)\n",
    "        output, _ = self.rnn(x)\n",
    "        real_event_h = get_last_event(output, masks)\n",
    "\n",
    "        # Forward pass with rev_x and rev_masks\n",
    "        rev_x = self.dropout(self.embedding(rev_x))\n",
    "        rev_x = sum_embeddings_with_masks(rev_x, rev_masks)\n",
    "        output_rev, _ = self.rnn(rev_x)\n",
    "        real_event_h_rev = get_last_event(output_rev, rev_masks)\n",
    "\n",
    "        # Concat both hidden states\n",
    "        logits = self.dropout(self.fc(torch.cat([real_event_h, real_event_h_rev], 1)))\n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs.view(batch_size)\n",
    "naive_rnn = NaiveRNN(num_items=len(itemidx))\n",
    "naive_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8689701094998521, 0.13102989050014796)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, num_majority, num_minority, K=0.01):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        \n",
    "        self.num_majority = num_majority\n",
    "        self.num_minority = num_minority\n",
    "        self.K = K\n",
    "    def forward(self, y_hat, y):\n",
    "        #y = torch.to('cpu').LongTensor(y)\n",
    "        delta_maj, delta_min = self.K / self.num_majority**(1/4), self.K / self.num_minority**(1/4)\n",
    "        zj = (y_hat > 0.5).int()\n",
    "        masks = torch.where(zj != y)\n",
    "        zc = y_hat\n",
    "        sigma_zc_maj = torch.exp(y_hat - delta_maj) / \\\n",
    "                            (torch.exp(y_hat - delta_maj) + torch.sum(torch.exp(zj[masks])))\n",
    "\n",
    "        sigma_zc_min = torch.exp(y_hat - delta_min) / \\\n",
    "                            (torch.exp(y_hat - delta_min) + torch.sum(torch.exp(zj[masks])))\n",
    "\n",
    "        loss = (- torch.log(sigma_zc_maj) - torch.log(sigma_zc_min)).mean()\n",
    "        cross_loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        return cross_loss\n",
    "minority = np.sum([element[-1] for element in train_dataset]) / len(train_dataset)\n",
    "majority = 1 - minority\n",
    "(majority, minority)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CustomLoss(num_majority=majority, num_minority=minority)\n",
    "\n",
    "optimizer = torch.optim.Adam(naive_rnn.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import *\n",
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.to(device).train()\n",
    "        train_loss = 0\n",
    "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
    "            x, masks, rev_x, rev_masks, y = x.to(device), masks.to(device), rev_x.to(device), rev_masks.to(device), y.to(device)\n",
    "            y_hat = model(x, masks, rev_x, rev_masks).view(y.shape[0])\n",
    "            loss = criterion(y_hat, y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        acc, auc, precision, recall, f1score = eval_model(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation acc: {:.2f}, auc:{:.2f}, precision: {:.2f}, recall: {:.2f}, f1: {:.2f}' \n",
    "              .format(epoch+1, acc, auc, precision, recall, f1score))    \n",
    "def classification_metrics(Y_score, Y_pred, Y_true):\n",
    "    acc, auc, precision, recall, f1score = accuracy_score(Y_true, Y_pred), \\\n",
    "                                           roc_auc_score(Y_true, Y_score), \\\n",
    "                                           precision_score(Y_true, Y_pred), \\\n",
    "                                           recall_score(Y_true, Y_pred), \\\n",
    "                                           f1_score(Y_true, Y_pred)\n",
    "    return acc, auc, precision, recall, f1score\n",
    "\n",
    "\n",
    "\n",
    "#precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "def eval_model(model, val_loader):\n",
    "    model.to(device).eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
    "        x, masks, rev_x, rev_masks, y = x.to(device), masks.to(device), rev_x.to(device), rev_masks.to(device), y.to(device)\n",
    "        y_true = y.long().detach().to('cpu')\n",
    "        y_hat = model(x, masks, rev_x, rev_masks).view(y.shape[0])\n",
    "        y_score = y_hat.detach().to('cpu')\n",
    "        y_pred = (y_hat > 0.5).int().detach().to('cpu')\n",
    "    acc, auc, precision, recall, f1score = classification_metrics(y_score, y_pred, y_true)\n",
    "    return acc, auc, precision, recall, f1score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 34.877946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:14<11:51, 14.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Validation acc: 0.46, auc:0.80, precision: 0.28, recall: 1.00, f1: 0.43\n",
      "Epoch: 2 \t Training Loss: 34.713399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:28<11:11, 14.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \t Validation acc: 0.67, auc:0.77, precision: 0.33, recall: 0.60, f1: 0.43\n",
      "Epoch: 3 \t Training Loss: 34.608937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:41<10:51, 13.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \t Validation acc: 0.71, auc:0.54, precision: 0.25, recall: 0.20, f1: 0.22\n",
      "Epoch: 4 \t Training Loss: 34.544980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:55<10:33, 13.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \t Validation acc: 0.62, auc:0.47, precision: 0.25, recall: 0.40, f1: 0.31\n",
      "Epoch: 5 \t Training Loss: 34.468515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [01:09<10:17, 13.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \t Validation acc: 0.58, auc:0.67, precision: 0.33, recall: 1.00, f1: 0.50\n",
      "Epoch: 6 \t Training Loss: 34.514550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [01:22<10:03, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \t Validation acc: 0.50, auc:0.66, precision: 0.27, recall: 0.80, f1: 0.40\n",
      "Epoch: 7 \t Training Loss: 34.501683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [01:36<09:49, 13.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \t Validation acc: 0.42, auc:0.79, precision: 0.26, recall: 1.00, f1: 0.42\n",
      "Epoch: 8 \t Training Loss: 34.478237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [01:50<09:36, 13.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 \t Validation acc: 0.46, auc:0.82, precision: 0.28, recall: 1.00, f1: 0.43\n",
      "Epoch: 9 \t Training Loss: 34.510086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [02:04<09:22, 13.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 \t Validation acc: 0.75, auc:0.66, precision: 0.43, recall: 0.60, f1: 0.50\n",
      "Epoch: 10 \t Training Loss: 34.376851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [02:17<09:07, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \t Validation acc: 0.54, auc:0.59, precision: 0.25, recall: 0.60, f1: 0.35\n",
      "Epoch: 11 \t Training Loss: 34.167537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [02:31<08:53, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 \t Validation acc: 0.46, auc:0.61, precision: 0.28, recall: 1.00, f1: 0.43\n",
      "Epoch: 12 \t Training Loss: 33.952574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [02:44<08:40, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 \t Validation acc: 0.62, auc:0.65, precision: 0.33, recall: 0.80, f1: 0.47\n",
      "Epoch: 13 \t Training Loss: 33.691611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [02:58<08:26, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 \t Validation acc: 0.62, auc:0.62, precision: 0.30, recall: 0.60, f1: 0.40\n",
      "Epoch: 14 \t Training Loss: 33.438576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [03:12<08:13, 13.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 \t Validation acc: 0.67, auc:0.75, precision: 0.33, recall: 0.60, f1: 0.43\n",
      "Epoch: 15 \t Training Loss: 33.282245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [03:26<07:59, 13.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 \t Validation acc: 0.79, auc:0.91, precision: 0.50, recall: 1.00, f1: 0.67\n",
      "Epoch: 16 \t Training Loss: 33.093085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [03:39<07:45, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 \t Validation acc: 0.88, auc:0.86, precision: 0.62, recall: 1.00, f1: 0.77\n",
      "Epoch: 17 \t Training Loss: 32.869023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [03:53<07:31, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 \t Validation acc: 0.67, auc:0.75, precision: 0.33, recall: 0.60, f1: 0.43\n",
      "Epoch: 18 \t Training Loss: 32.888567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [04:07<07:18, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 \t Validation acc: 0.62, auc:0.82, precision: 0.33, recall: 0.80, f1: 0.47\n",
      "Epoch: 19 \t Training Loss: 32.756325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [04:20<07:02, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 \t Validation acc: 0.79, auc:0.85, precision: 0.50, recall: 1.00, f1: 0.67\n",
      "Epoch: 20 \t Training Loss: 32.683367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [04:34<06:48, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 \t Validation acc: 0.67, auc:0.78, precision: 0.36, recall: 0.80, f1: 0.50\n",
      "Epoch: 21 \t Training Loss: 32.581540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [04:47<06:35, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 \t Validation acc: 0.83, auc:0.98, precision: 0.56, recall: 1.00, f1: 0.71\n",
      "Epoch: 22 \t Training Loss: 32.417383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [05:01<06:21, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 \t Validation acc: 0.83, auc:0.97, precision: 0.56, recall: 1.00, f1: 0.71\n",
      "Epoch: 23 \t Training Loss: 32.469156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [05:15<06:08, 13.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 \t Validation acc: 0.71, auc:0.91, precision: 0.42, recall: 1.00, f1: 0.59\n",
      "Epoch: 24 \t Training Loss: 32.378585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [05:28<05:55, 13.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 \t Validation acc: 0.96, auc:0.95, precision: 0.83, recall: 1.00, f1: 0.91\n",
      "Epoch: 25 \t Training Loss: 32.399429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [05:42<05:40, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 \t Validation acc: 0.79, auc:0.91, precision: 0.50, recall: 1.00, f1: 0.67\n",
      "Epoch: 26 \t Training Loss: 32.295546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [05:56<05:27, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 \t Validation acc: 0.88, auc:0.96, precision: 0.62, recall: 1.00, f1: 0.77\n",
      "Epoch: 27 \t Training Loss: 32.263115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [06:09<05:13, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 \t Validation acc: 0.75, auc:0.96, precision: 0.45, recall: 1.00, f1: 0.62\n",
      "Epoch: 28 \t Training Loss: 32.149071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [06:23<05:00, 13.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 \t Validation acc: 0.71, auc:0.96, precision: 0.42, recall: 1.00, f1: 0.59\n",
      "Epoch: 29 \t Training Loss: 32.203840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [06:37<04:47, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 \t Validation acc: 0.83, auc:0.97, precision: 0.56, recall: 1.00, f1: 0.71\n",
      "Epoch: 30 \t Training Loss: 32.114401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [06:50<04:33, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 \t Validation acc: 0.75, auc:0.83, precision: 0.43, recall: 0.60, f1: 0.50\n",
      "Epoch: 31 \t Training Loss: 32.153026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [07:04<04:20, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 \t Validation acc: 0.92, auc:0.97, precision: 0.71, recall: 1.00, f1: 0.83\n",
      "Epoch: 32 \t Training Loss: 32.114319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [07:18<04:06, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 \t Validation acc: 0.79, auc:0.97, precision: 0.50, recall: 1.00, f1: 0.67\n",
      "Epoch: 33 \t Training Loss: 32.158764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [07:31<03:52, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 \t Validation acc: 0.83, auc:0.91, precision: 0.56, recall: 1.00, f1: 0.71\n",
      "Epoch: 34 \t Training Loss: 32.030841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [07:45<03:39, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 \t Validation acc: 0.83, auc:0.85, precision: 0.60, recall: 0.60, f1: 0.60\n",
      "Epoch: 35 \t Training Loss: 32.099155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [07:59<03:25, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 \t Validation acc: 0.83, auc:0.91, precision: 0.56, recall: 1.00, f1: 0.71\n",
      "Epoch: 36 \t Training Loss: 32.035524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [08:12<03:11, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 \t Validation acc: 0.92, auc:0.85, precision: 0.80, recall: 0.80, f1: 0.80\n",
      "Epoch: 37 \t Training Loss: 32.045492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [08:26<02:57, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 \t Validation acc: 0.88, auc:0.93, precision: 0.67, recall: 0.80, f1: 0.73\n",
      "Epoch: 38 \t Training Loss: 31.912011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [08:40<02:44, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 \t Validation acc: 0.92, auc:0.95, precision: 0.71, recall: 1.00, f1: 0.83\n",
      "Epoch: 39 \t Training Loss: 31.969549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [08:54<02:30, 13.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 \t Validation acc: 0.96, auc:0.96, precision: 0.83, recall: 1.00, f1: 0.91\n",
      "Epoch: 40 \t Training Loss: 31.941683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [09:07<02:17, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 \t Validation acc: 0.88, auc:0.95, precision: 0.62, recall: 1.00, f1: 0.77\n",
      "Epoch: 41 \t Training Loss: 31.943736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [09:21<02:03, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 \t Validation acc: 0.92, auc:0.96, precision: 0.71, recall: 1.00, f1: 0.83\n",
      "Epoch: 42 \t Training Loss: 31.862036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [09:35<01:49, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 \t Validation acc: 0.92, auc:0.92, precision: 0.71, recall: 1.00, f1: 0.83\n",
      "Epoch: 43 \t Training Loss: 31.928226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [09:48<01:35, 13.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 \t Validation acc: 0.92, auc:0.94, precision: 0.80, recall: 0.80, f1: 0.80\n",
      "Epoch: 44 \t Training Loss: 31.890965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [10:02<01:21, 13.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 \t Validation acc: 0.83, auc:0.95, precision: 0.56, recall: 1.00, f1: 0.71\n",
      "Epoch: 45 \t Training Loss: 31.895241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [10:16<01:08, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 \t Validation acc: 0.79, auc:0.84, precision: 0.50, recall: 0.80, f1: 0.62\n",
      "Epoch: 46 \t Training Loss: 31.921272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [10:29<00:54, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 \t Validation acc: 0.79, auc:0.88, precision: 0.50, recall: 0.80, f1: 0.62\n",
      "Epoch: 47 \t Training Loss: 31.921439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [10:43<00:41, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 \t Validation acc: 0.83, auc:0.94, precision: 0.57, recall: 0.80, f1: 0.67\n",
      "Epoch: 48 \t Training Loss: 31.928663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [10:57<00:27, 13.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 \t Validation acc: 0.75, auc:0.92, precision: 0.45, recall: 1.00, f1: 0.62\n",
      "Epoch: 49 \t Training Loss: 31.799830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [11:11<00:13, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 \t Validation acc: 0.75, auc:0.96, precision: 0.45, recall: 1.00, f1: 0.62\n",
      "Epoch: 50 \t Training Loss: 31.883884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [11:24<00:00, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 \t Validation acc: 0.88, auc:0.96, precision: 0.62, recall: 1.00, f1: 0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 50\n",
    "train(naive_rnn, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875 0.9578947368421054 0.625 1.0 <function f1_score at 0x7fe3b2b138b0>\n"
     ]
    }
   ],
   "source": [
    "acc, auc, precision, recall, f1score = eval_model(naive_rnn, val_loader)\n",
    "print(acc, auc, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875 0.9578947368421054 0.625 1.0 <function f1_score at 0x7fe3b2b138b0>\n"
     ]
    }
   ],
   "source": [
    "acc, auc, precision, recall, f1score = eval_model(naive_rnn, test_loader)\n",
    "print(acc, auc, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

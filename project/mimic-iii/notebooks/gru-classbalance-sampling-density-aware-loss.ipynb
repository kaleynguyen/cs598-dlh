{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting polars\n",
      "  Downloading polars-0.17.12-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.9/17.9 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in ./.local/lib/python3.8/site-packages (1.23.5)\n",
      "Requirement already satisfied: torch in /usr/lib/python3/dist-packages (1.13.1)\n",
      "Installing collected packages: polars\n",
      "Successfully installed polars-0.17.12\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install polars numpy torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# set seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "DATA_PATH = \"\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "obj = pickle.load(open(os.path.join(DATA_PATH, 'allevents_by_episode_48_charts'), 'rb'))\n",
    "patients = obj.select(pl.col('subject_id')).to_series().to_list()\n",
    "seqs = obj.select(pl.col('itemidx')).to_series().to_list()\n",
    "mortality = obj.select(pl.col('mortality_tf')).to_series().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40119"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemidx = set([each_item for events in seqs for items in events for each_item in items])\n",
    "len(itemidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, seqs, mortality):\n",
    "        self.seqs = seqs\n",
    "        self.labels = mortality\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, index):\n",
    "        return self.seqs[index], self.labels[index]\n",
    "dataset = CustomDataset(seqs, mortality)\n",
    "assert len(dataset) == len(obj) #TODO write test separately \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function\n",
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Input data: a tuple of seqs, and label\n",
    "    Output: x (num_patients, num_events, num_itemids) with *real* items in the events\n",
    "            rev_x (num_patients, num_events, num_itemids) with *reverse real* items in the events\n",
    "            masks  (num_patients, num_events, num_itemids) whether a *real* itemid in the events is present\n",
    "            rev_masks (num_patients, num_events, num_itemids) whether a *reverse real* itemid in the events is present\n",
    "    \"\"\"\n",
    "    seqs, labels = zip(*data)\n",
    "    num_patients = len(seqs)\n",
    "    max_num_events = max([len(event) for event in seqs])\n",
    "    max_num_items = max([len(itemid) for event in seqs for itemid in event])\n",
    "    tensor_shape = (num_patients, max_num_events, max_num_items)\n",
    "    x =        torch.zeros(tensor_shape, dtype=torch.long)\n",
    "    rev_x =    torch.zeros(tensor_shape, dtype=torch.long)\n",
    "    masks =    torch.zeros(tensor_shape, dtype=torch.bool)\n",
    "    rev_masks = torch.zeros(tensor_shape, dtype=torch.bool) \n",
    "    y =        torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "\n",
    "    for i_patient, events in enumerate(seqs):\n",
    "        for i_event, item in enumerate(events):\n",
    "            padded_item = torch.concat([torch.tensor(item),\n",
    "                                        torch.zeros(max_num_items - len(item))]).long()\n",
    "            x[i_patient, i_event, :] = padded_item\n",
    "            masks[i_patient, i_event, :] = torch.where(padded_item!=0,1,0)  \n",
    "    for i_patient, events in enumerate(seqs):\n",
    "        idx_all_real_events = torch.sum(x[i_patient, :, :], dim=(1))!= 0\n",
    "        idx_padded_events =torch.sum(x[i_patient, :, :], dim=(1))== 0\n",
    "        fliped = torch.flip(x[i_patient, idx_all_real_events, :].unsqueeze(1), (0,)).squeeze(1)\n",
    "        rev_x[i_patient, :, :] = torch.concat((fliped, x[i_patient, idx_padded_events, :] ))\n",
    "        rev_masks[i_patient, :, :] = torch.where(rev_x[i_patient, :, :] != 0, True, False)\n",
    "    return x, masks, rev_x, rev_masks, y\n",
    "\n",
    "    \n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(dataset, batch_size=10, collate_fn=collate_fn, pin_memory=True)\n",
    "loader_iter = iter(loader)\n",
    "x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "#assert x.shape == masks.shape == (10, 8, 313)\n",
    "assert y.shape == (10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 27032\n",
      "Length of train with mortality: 3591\n",
      "Length of train with survive patients: 23441\n",
      "Length of val dataset: 3379\n",
      "Length of val with mortality: 418\n",
      "Length of val with survive patients: 26614\n",
      "Length of test dataset: 3379\n",
      "Length of test with mortality: 462\n",
      "Length of test with survive patients: 2917\n"
     ]
    }
   ],
   "source": [
    "train, val, test = int(len(dataset)*0.8), int(len(dataset)*0.1), len(dataset) - int(len(dataset)*0.8) -  int(len(dataset)*0.1)\n",
    "lengths = [train, val, test]\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset=dataset, lengths=lengths)\n",
    "from pytorch_metric_learning import samplers\n",
    "\n",
    "def load_data(train_dataset, val_dataset, test_dataset, collate_fn):\n",
    "    batch_size = 64\n",
    "    seqs, labels = zip(*train_dataset)\n",
    "    sampler = samplers.MPerClassSampler(labels, m=len(train_dataset)//2, batch_size=None, length_before_new_iter=1000)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, \\\n",
    "                                collate_fn=collate_fn, \\\n",
    "                                num_workers=1, sampler=sampler)\n",
    "    \n",
    "    val_loader = DataLoader(train_dataset, batch_size=batch_size, \\\n",
    "                                collate_fn=collate_fn, \\\n",
    "                                num_workers=4, shuffle=False)\n",
    "    test_loader = DataLoader(train_dataset, batch_size=batch_size, \\\n",
    "                                collate_fn=collate_fn, \\\n",
    "                                num_workers=4, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset, test_dataset, collate_fn)\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of train with mortality:\", np.sum([element[-1] for element in train_dataset]))\n",
    "print(\"Length of train with survive patients:\", (len(train_dataset) - np.sum([element[-1] for element in train_dataset])))\n",
    "print(\"Length of val dataset:\", len(val_dataset))\n",
    "print(\"Length of val with mortality:\", np.sum([element[-1] for element in val_dataset]))\n",
    "print(\"Length of val with survive patients:\", (len(train_dataset) - np.sum([element[-1] for element in val_dataset])))\n",
    "print(\"Length of test dataset:\", len(test_dataset))\n",
    "print(\"Length of test with mortality:\", np.sum([element[-1] for element in test_dataset]))\n",
    "print(\"Length of test with survive patients:\", (len(test_dataset) - np.sum([element[-1] for element in test_dataset])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13516, 13516)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_for_loop(iterable):\n",
    "    iterator = iter(iterable)\n",
    "    done_looping = False\n",
    "    count_mor, count_sur = 0, 0\n",
    "    while not done_looping:\n",
    "        try:\n",
    "            item = next(iterator)\n",
    "        except StopIteration:\n",
    "            done_looping = True\n",
    "        else:\n",
    "            x, masks, rev_x, rev_masks, y = item\n",
    "            count_mor += y.sum().item()\n",
    "            count_sur += len(y) -  y.sum().item()\n",
    "    return count_mor, count_sur\n",
    "\n",
    "custom_for_loop(train_loader)# == len(train_dataset) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_masks(x, masks):\n",
    "    \"\"\"\n",
    "    Input:  x               (batch_size, num_events, num_itemids, embedding_dims)\n",
    "            mask            (batch_size, num_events, num_itemids)\n",
    "    Output: sum_embeddings  (batch_size, num_events, embedding_dims)\n",
    "    The return output compress the num_itemids into embedding vectors\n",
    "    \"\"\"\n",
    "    masks = masks.unsqueeze(-1).expand(x.shape[0], x.shape[1], x.shape[2], x.shape[3])\n",
    "    return torch.sum(masks * x, 2)\n",
    "def get_last_event(hidden_states, masks):\n",
    "    \"\"\"\n",
    "    hidden_states: (batch_size, #item, embedding_dim)\n",
    "    masks:         (batch_size, #item, embedding_dim)\n",
    "    return last_hidden_state: (batch_size, embedding_dim)\n",
    "    \"\"\"\n",
    "    idx_last_event = torch.argmin(torch.sum(masks, 2), 1)\n",
    "    return hidden_states[torch.arange(hidden_states.shape[0]), torch.where(idx_last_event - 1 < 0, max(idx_last_event), idx_last_event - 1),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = torch.tensor([[\n",
    "    [-0.8201, 0.3956, 0.8989, -1.3884, -0.1670, 0.2851, -0.6411],\n",
    "    [-0.8937, 0.9265, -0.5355, -1.1597, -0.4602, 0.7085, 1.0128],\n",
    "    [ 0.2304, 1.0902, -1.5827, -0.3246, 1.9264, -0.3300, 0.1984]],\n",
    "   \n",
    "[[ 0.7821, 1.0391, -0.7245, -0.2093, -0.2153, -1.8157, -0.3452],\n",
    "    [-2.0615, 0.6741, -1.3233, -1.3598, -0.0835, -0.0235, 0.1744],\n",
    "    [ 2.2983, 0.9571, -0.6619, -0.8285, -0.6057, -1.4013, 1.2973]],\n",
    "\n",
    "   [[ 1.6409, -1.0567, -0.2616, -0.2501, 0.5011, 0.2600, -0.1782],\n",
    "    [    -0.2595, -0.0145, -0.3839, -2.9662, -1.0606, -0.3090, 0.9343],\n",
    "    [ 1.6243, 0.0016, -0.4375, -2.1085, 1.1450, -0.3822, -0.3553]],\n",
    "\n",
    "   [[ 0.7542, 0.1332, 0.1825, -0.5146, 0.8005, -0.1259, -0.9578],\n",
    "    [ 1.7518, 0.9796, 0.4105, 1.7675, -0.0832, 0.5087, -0.8253],\n",
    "    [ 0.1633, 0.5013, 1.4206, 1.1542, -1.5366, -0.5577, -0.4383]]])\n",
    "masks = torch.tensor([[[ True,  True,  True,  True, False],\n",
    "         [ True,  True,  True, False, False],\n",
    "         [False, False, False, False, False]],\n",
    "\n",
    "        [[ True, False, False, False, False],\n",
    "         [ True,  True,  True,  True, False],\n",
    "         [ True,  True, False, False, False]],\n",
    "\n",
    "        [[ True,  True, False, False, False],\n",
    "         [False, False, False, False, False],\n",
    "         [False, False, False, False, False]],\n",
    "\n",
    "        [[ True,  True,  True, False, False],\n",
    "         [ True,  True,  True, False, False],\n",
    "         [False, False, False, False, False]]])\n",
    "assert torch.sum(get_last_event(ts, masks) == torch.tensor([\n",
    "    [-0.8937, 0.9265, -0.5355, -1.1597, -0.4602, 0.7085, 1.0128],\n",
    "    [ 2.2983, 0.9571, -0.6619, -0.8285, -0.6057, -1.4013, 1.2973],\n",
    "    [1.6409, -1.0567, -0.2616, -0.2501, 0.5011, 0.2600, -0.1782],\n",
    "    [ 1.7518, 0.9796, 0.4105, 1.7675, -0.0832, 0.5087, -0.8253],\n",
    "    ])) == 7*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaiveRNN(\n",
       "  (embedding): Embedding(40119, 128)\n",
       "  (rnn): GRU(128, 128, batch_first=True)\n",
       "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive RNN hehe :P\n",
    "class NaiveRNN(nn.Module):\n",
    "    def __init__(self, num_items):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_items, embedding_dim=128)\n",
    "        self.rnn = nn.GRU(128, hidden_size=128, batch_first=True)\n",
    "        self.rev_rnn = nn.GRU(128, hidden_size=128, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=128*2, out_features=1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        batch_size = x.shape[0]\n",
    "        # Forward pass with x and masks\n",
    "        x = self.dropout(self.embedding(x))\n",
    "        x = sum_embeddings_with_masks(x, masks)\n",
    "        output, _ = self.rnn(x)\n",
    "        real_event_h = get_last_event(output, masks)\n",
    "\n",
    "        # Forward pass with rev_x and rev_masks\n",
    "        rev_x = self.dropout(self.embedding(rev_x))\n",
    "        rev_x = sum_embeddings_with_masks(rev_x, rev_masks)\n",
    "        output_rev, _ = self.rnn(rev_x)\n",
    "        real_event_h_rev = get_last_event(output_rev, rev_masks)\n",
    "\n",
    "        # Concat both hidden states\n",
    "        logits = self.dropout(self.fc(torch.cat([real_event_h, real_event_h_rev], 1)))\n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs.view(batch_size)\n",
    "naive_rnn = NaiveRNN(num_items=len(itemidx))\n",
    "naive_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8671574430304824, 0.1328425569695176)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, num_majority, num_minority, K=0.01):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        \n",
    "        self.num_majority = num_majority\n",
    "        self.num_minority = num_minority\n",
    "        self.K = K\n",
    "    def forward(self, y_hat, y):\n",
    "        #y = torch.to('cpu').LongTensor(y)\n",
    "        delta_maj, delta_min = self.K / self.num_majority**(1/4), self.K / self.num_minority**(1/4)\n",
    "        zj = (y_hat > 0.5).int()\n",
    "        masks = torch.where(zj != y)\n",
    "        zc = y_hat\n",
    "        sigma_zc_maj = torch.exp(y_hat - delta_maj) / \\\n",
    "                            (torch.exp(y_hat - delta_maj) + torch.sum(torch.exp(zj[masks])))\n",
    "\n",
    "        sigma_zc_min = torch.exp(y_hat - delta_min) / \\\n",
    "                            (torch.exp(y_hat - delta_min) + torch.sum(torch.exp(zj[masks])))\n",
    "\n",
    "        loss = (- torch.log(sigma_zc_maj) - torch.log(sigma_zc_min)).mean()\n",
    "        return loss\n",
    "minority = np.sum([element[-1] for element in train_dataset]) / len(train_dataset)\n",
    "majority = 1 - minority\n",
    "(majority, minority)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CustomLoss(num_majority=majority, num_minority=minority)\n",
    "optimizer = torch.optim.Adam(naive_rnn.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import *\n",
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.to(device).train()\n",
    "        train_loss = 0\n",
    "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
    "            x, masks, rev_x, rev_masks, y = x.to(device), masks.to(device), rev_x.to(device), rev_masks.to(device), y.to(device)\n",
    "            y_hat = model(x, masks, rev_x, rev_masks).view(y.shape[0])\n",
    "            loss = criterion(y_hat, y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        acc, auc, precision, recall, f1score = eval_model(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation acc: {:.2f}, auc:{:.2f}, precision: {:.2f}, recall: {:.2f}, f1: {:.2f}' \n",
    "              .format(epoch+1, acc, auc, precision, recall, f1score))    \n",
    "def classification_metrics(Y_score, Y_pred, Y_true):\n",
    "    acc, auc, precision, recall, f1score = accuracy_score(Y_true, Y_pred), \\\n",
    "                                           roc_auc_score(Y_true, Y_score), \\\n",
    "                                           precision_score(Y_true, Y_pred), \\\n",
    "                                           recall_score(Y_true, Y_pred), \\\n",
    "                                           f1_score(Y_true, Y_pred)\n",
    "    return acc, auc, precision, recall, f1score\n",
    "\n",
    "\n",
    "\n",
    "#precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "def eval_model(model, val_loader):\n",
    "    model.to(device).eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
    "        x, masks, rev_x, rev_masks, y = x.to(device), masks.to(device), rev_x.to(device), rev_masks.to(device), y.to(device)\n",
    "        y_true = y.long().detach().to('cpu')\n",
    "        y_hat = model(x, masks, rev_x, rev_masks).view(y.shape[0])\n",
    "        y_score = y_hat.detach().to('cpu')\n",
    "        y_pred = (y_hat > 0.5).int().detach().to('cpu')\n",
    "    acc, auc, precision, recall, f1score = classification_metrics(y_score, y_pred, y_true)\n",
    "    return acc, auc, precision, recall, f1score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 156.513580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:15<12:59, 15.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Validation acc: 0.12, auc:0.73, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 2 \t Training Loss: 155.709984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:30<12:08, 15.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \t Validation acc: 0.12, auc:0.75, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 3 \t Training Loss: 155.312076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:45<11:44, 14.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \t Validation acc: 0.12, auc:0.79, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 4 \t Training Loss: 156.270667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [01:00<11:24, 14.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \t Validation acc: 0.12, auc:0.71, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 5 \t Training Loss: 155.296081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [01:14<11:05, 14.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \t Validation acc: 0.12, auc:0.68, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 6 \t Training Loss: 155.637665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [01:29<10:49, 14.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \t Validation acc: 0.12, auc:0.76, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 7 \t Training Loss: 155.536051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [01:44<10:33, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \t Validation acc: 0.12, auc:0.75, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 8 \t Training Loss: 154.812604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [01:58<10:17, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 \t Validation acc: 0.12, auc:0.70, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 9 \t Training Loss: 155.650688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [02:13<10:03, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 \t Validation acc: 0.12, auc:0.75, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 10 \t Training Loss: 155.524239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [02:28<09:48, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \t Validation acc: 0.12, auc:0.76, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 11 \t Training Loss: 155.651887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [02:42<09:33, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 \t Validation acc: 0.12, auc:0.71, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 12 \t Training Loss: 155.291520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [02:57<09:18, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 \t Validation acc: 0.12, auc:0.70, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 13 \t Training Loss: 155.849642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [03:12<09:03, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 \t Validation acc: 0.12, auc:0.68, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 14 \t Training Loss: 156.050639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [03:26<08:49, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 \t Validation acc: 0.12, auc:0.68, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 15 \t Training Loss: 155.930221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [03:41<08:33, 14.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 \t Validation acc: 0.12, auc:0.71, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 16 \t Training Loss: 156.084717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [03:56<08:18, 14.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 \t Validation acc: 0.12, auc:0.70, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 17 \t Training Loss: 156.130780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [04:10<08:04, 14.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 \t Validation acc: 0.12, auc:0.73, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 18 \t Training Loss: 156.027644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [04:25<07:49, 14.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 \t Validation acc: 0.12, auc:0.71, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 19 \t Training Loss: 155.116654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [04:40<07:35, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 \t Validation acc: 0.12, auc:0.70, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 20 \t Training Loss: 155.627605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [04:54<07:21, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 \t Validation acc: 0.12, auc:0.67, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 21 \t Training Loss: 155.600249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [05:09<07:05, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 \t Validation acc: 0.12, auc:0.73, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 22 \t Training Loss: 156.321629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [05:24<06:50, 14.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 \t Validation acc: 0.12, auc:0.71, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 23 \t Training Loss: 155.110245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [05:38<06:35, 14.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 \t Validation acc: 0.12, auc:0.67, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 24 \t Training Loss: 154.641174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [05:53<06:21, 14.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 \t Validation acc: 0.12, auc:0.71, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 25 \t Training Loss: 154.907610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [06:08<06:06, 14.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 \t Validation acc: 0.12, auc:0.65, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 26 \t Training Loss: 155.302103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [06:22<05:52, 14.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 \t Validation acc: 0.12, auc:0.71, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 27 \t Training Loss: 155.543331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [06:37<05:37, 14.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 \t Validation acc: 0.12, auc:0.70, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 28 \t Training Loss: 156.445303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [06:52<05:22, 14.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 \t Validation acc: 0.12, auc:0.75, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 29 \t Training Loss: 155.607108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [07:06<05:07, 14.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 \t Validation acc: 0.12, auc:0.71, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 30 \t Training Loss: 155.910581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [07:21<04:53, 14.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 \t Validation acc: 0.12, auc:0.73, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 31 \t Training Loss: 154.366926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [07:36<04:39, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 \t Validation acc: 0.12, auc:0.70, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 32 \t Training Loss: 155.331910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [07:51<04:24, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 \t Validation acc: 0.12, auc:0.71, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 33 \t Training Loss: 155.317813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [08:05<04:09, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 \t Validation acc: 0.12, auc:0.70, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 34 \t Training Loss: 155.800599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [08:20<03:55, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 \t Validation acc: 0.12, auc:0.71, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 35 \t Training Loss: 155.687588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [08:35<03:40, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 \t Validation acc: 0.12, auc:0.67, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 36 \t Training Loss: 155.840820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [08:49<03:25, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 \t Validation acc: 0.12, auc:0.63, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 37 \t Training Loss: 155.075115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [09:04<03:11, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 \t Validation acc: 0.12, auc:0.75, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 38 \t Training Loss: 155.865674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [09:19<02:56, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 \t Validation acc: 0.12, auc:0.67, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 39 \t Training Loss: 155.756726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [09:34<02:42, 14.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 \t Validation acc: 0.12, auc:0.71, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 40 \t Training Loss: 155.707227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [09:48<02:27, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 \t Validation acc: 0.12, auc:0.63, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 41 \t Training Loss: 155.453489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [10:03<02:12, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 \t Validation acc: 0.12, auc:0.71, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 42 \t Training Loss: 155.300930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [10:18<01:57, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 \t Validation acc: 0.12, auc:0.70, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 43 \t Training Loss: 156.630456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [10:32<01:42, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 \t Validation acc: 0.12, auc:0.70, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 44 \t Training Loss: 155.071238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [10:47<01:28, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 \t Validation acc: 0.12, auc:0.67, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 45 \t Training Loss: 155.606615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [11:02<01:13, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 \t Validation acc: 0.12, auc:0.71, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 46 \t Training Loss: 155.145986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [11:17<00:58, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 \t Validation acc: 0.12, auc:0.75, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 47 \t Training Loss: 156.360177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [11:31<00:44, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 \t Validation acc: 0.12, auc:0.71, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 48 \t Training Loss: 155.340698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [11:46<00:29, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 \t Validation acc: 0.12, auc:0.71, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 49 \t Training Loss: 156.400750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [12:01<00:14, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 \t Validation acc: 0.12, auc:0.70, precision: 0.12, recall: 1.00, f1: 0.22\n",
      "Epoch: 50 \t Training Loss: 155.861536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [12:15<00:00, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 \t Validation acc: 0.12, auc:0.68, precision: 0.12, recall: 1.00, f1: 0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 50\n",
    "train(naive_rnn, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125 0.6825396825396826 0.125 1.0 <function f1_score at 0x7fc514ae8ee0>\n"
     ]
    }
   ],
   "source": [
    "acc, auc, precision, recall, f1score = eval_model(naive_rnn, val_loader)\n",
    "print(acc, auc, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125 0.6825396825396826 0.125 1.0 <function f1_score at 0x7fc514ae8ee0>\n"
     ]
    }
   ],
   "source": [
    "acc, auc, precision, recall, f1score = eval_model(naive_rnn, test_loader)\n",
    "print(acc, auc, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
